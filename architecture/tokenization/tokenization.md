1. compression ratio(å‹ç¼©æ¯”)
åŸæ–‡çš„å­—èŠ‚æ•°ï¼ˆå­—ç¬¦æ•°ï¼‰/ encodingåçš„tokenæ•°

2. tokenizeråˆ†ç±»
a. Character-based tokenization
ä¸€ä¸ªunicodeå­—ç¬¦å¯¹åº”ä¸€ä¸ªç ç‚¹ï¼Œå°±æ˜¯ä¸€ä¸ªtoken
å¦‚æœå‹ç¼©æ¯”ï¼ˆå­—èŠ‚è®¡ç®—ï¼‰ï¼Œåˆ™å‹ç¼©æ¯”è¿˜å¯ä»¥ï¼Œä½†æ˜¯ï¼š
é—®é¢˜ 1ï¼šè¯æ±‡é‡è¿‡å¤§ã€‚
é—®é¢˜ 2ï¼šè®¸å¤šå­—ç¬¦ååˆ†ç½•è§ï¼ˆä¾‹å¦‚ï¼šğŸŒï¼‰ï¼Œè¿™é€ æˆäº†è¯æ±‡è¡¨çš„ä½æ•ˆåˆ©ç”¨ã€‚

b. Byte-based tokenization
åŸºäºutf-8æŠŠtextè½¬ä¸ºä¸€ä¸ªbyte çš„listï¼Œç„¶åä¸€ä¸ªbytexè½¬ä¸ºä¸€ä¸ªæ•°å­—ï¼Œæœ€åè¯è¡¨åªæœ‰256ä¸ªå€¼ã€‚
å¥½å¤„ï¼šè¯è¡¨å°
åå¤„ï¼šThe compression ratio is terrible, which means the sequences will be too long.ï¼ˆå°¤å…¶æ˜¯attentionï¼Œæ—¶é—´å¤æ‚åº¦æ˜¯n^2)

c. Word-based tokenization
problems: 
(1)The number of words is huge (like for Unicode characters).  
(2)Many words are rare and the model won't learn much about them.
(3)This doesn't obviously provide a fixed vocabulary size.
(4)New words we haven't seen during training get a special UNK token, which is ugly and can mess up perplexity calculations.
å‹ç¼©æ¯”è‚¯å®šé«˜

d. Byte Pair Encoding
start with each byte as a token, and successively merge the most common pair of adjacent tokens.
å‹ç¼©æ¯”è¿˜ä¸é”™ï¼Œè¯è¡¨ä¹Ÿä¸ä¼šè¿‡å¤§