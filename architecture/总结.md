整体看下来，一个结构可以被接受用来优化transformer，无非就是
loss更好收敛、更稳定（更容易收敛：通常是指在训练过程中，loss下降更快、更平滑，不容易震荡或发散。更稳定：强调的是训练过程中梯度不会爆炸/消失，loss曲线不大幅波动，长期都能维持在合理的下降轨迹。）
用更少的参数（减少内存），训练起来更快，更省资源
更稳定（比如没有尖刺、爆炸这些）